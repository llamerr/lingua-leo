Modelle bewerten

Hinweis: AutoML Translation-Funktionen werden sowohl von der AutoML API als auch von der Cloud Translation – Advanced API angeboten. Wir empfehlen, die Cloud Translation – Advanced API zum Erstellen von Datasets und Modellen zu verwenden, da zukünftige Verbesserungen nur für die Cloud Translation – Advanced API gelten werden. Weitere Informationen finden Sie unter AutoML-Ressourcen aktualisieren.

Nach dem Training eines Modells verwendet AutoML Translation Elemente aus dem Set TEST, um die Qualität und Treffsicherheit des neuen Modells zu bewerten. AutoML Translation ermittelt die Modellqualität. Es nutzt dazu den BLEU-Score (Bilingual Evaluation Understudy) der angibt, wie ähnlich der Kandidatentext den Referenztexten ist, wobei Werte, die näher an 1 liegen, ähnlichere Texte darstellen.

Mithilfe des BLEU-Scores kann auch die Modellqualität insgesamt bewertet werden. Außerdem können Sie die Modellausgabe im Hinblick auf bestimmte Datenelemente bewerten. Exportieren Sie dazu das Set TEST mit den Modellvorhersagen. Die exportierten Daten enthalten sowohl den Referenztext (aus dem Original-Dataset) als auch den Kandidatentext des Modells.

Mit diesen Daten können Sie nun bewerten, ob Ihr Modell einsatzbereit ist. Wenn Sie mit der Qualität nicht zufrieden sind, fügen Sie am besten weitere (und stärker unterschiedliche) Paare von Trainingssätzen hinzu. Die eine Möglichkeit besteht also darin, weitere Satzpaare hinzuzufügen. Verwenden Sie dazu den Link Dateien hinzufügen in der Titelleiste. Nachdem Sie Dateien hinzugefügt haben, erstellen Sie ein neues Modell. Klicken Sie hierzu auf der Seite Trainieren auf die Schaltfläche Neues Modell trainieren. Wiederholen Sie diesen Vorgang, bis Sie eine ausreichend hohe Qualität erreicht haben.